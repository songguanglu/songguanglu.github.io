
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="keywords" content="Guanglu Song, SenseTime X-Lab, 宋广录" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="cv/cv_en.pdf">Resume(简历)</a></div>
<div class="menu-item"><a href="mailto:guanglusong@foxmail.com">Email</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Guanglu Song </h1>
</div>
<table class="imgtable"><tr><td>
<img src="me.jpg" alt="Guanglu Song"  height="200px" />&nbsp;</td>
<td align="left"><p>Director of the Base Model R&D Department and the Base Model Services Department,
<a href="https://www.sensetime.com" target=_blank>SenseTime</a><br />
guanglusong@foxmail.com <br />
<a href="https://scholar.google.com/citations?user=Bd3v08QAAAAJ&hl=zh-CN" target=_blank>Google Scholar</a><br />
<a href="cv/cv_en.pdf" target=_blank>Resume</a><br />
</td></tr></table>


<h2>News</h2>
<ul>
  <li><p>[2024] Our AIGC product <font color="#FF0000">MiaoHua (QuPai)</font> has garnered users over 4,000,000, with a DAU exceeding 530,000.</p>
</li>
  <li><p>[2024] <font color="#FF0000">12 papers</font> are accepted by ECCV/CVPR/NeurIPS.</p>
</li>
<li><p>[2023] <font color="#FF0000">7 papers</font> are accepted by TPAMI/ICCV.</p>
</li>
<li><p>[2022] <font color="#FF0000">7 paper</font> are accepted by ECCV/ICLR/NeurIPS.</p>
</li>
<li><p>[2021] We obtain the <font color="#FF0000">Top-1</font> of <a href="https://insightface.ai/mfr21">ICCV2021-MFR Glint360K Track</a>, <font color="#FF0000">Top-1</font> of <a href="https://insightface.ai/mfr21">ICCV2021-MFR Unconstrained Track</a> and <font color="#FF0000">Top-1</font> of <a href="https://www.face-benchmark.org/challenge.html">ICCV2021-MFR WebFace260M Track</a></p>
</li>
<li><p>[2021] We obtain the <font color="#FF0000">Top-1</font> of <a href="https://pages.nist.gov/frvt/html/frvt1N.html">NIST FRVT 1:N Identification</a>, an official 1:N face recognition algorithms evaluation platform</p>
</li>
<li><p>[2021] We obtain the <font color="#FF0000">Top-1</font> of <a href="https://pages.nist.gov/frvt/html/frvt11.html">NIST FRVT 1:1 Verification</a>, an official 1:1 face recognition algorithms evaluation platform</p>
</li>
<li><p>[2021] We obtain the <font color="#FF0000">Top-1</font> of <a href="https://pages.nist.gov/frvt/html/frvt_facemask.html">NIST FRVT Face Mask Effects</a>, an official evaluation face recognition accuracy with face masks</p>
</li>
<li><p>[2017-2021] <font color="#FF0000">7 papers</font> are accepted by CVPR, ECCV, AAAI, ICCV.</p>
</li>
<li><p>[2020] We obtain the <font color="#FF0000">Top-1</font> of <a href="http://activity-net.org/challenges/2020/challenge.html">AcitivityNet Challenge 2020</a> [<a href="https://arxiv.org/pdf/2006.09116.pdf">Solutions</a>]</p>
</li>
<li><p>[2019] We obtain the <font color="#FF0000">Top-1</font> of <a href="http://moments.csail.mit.edu/challenge_iccv_2019.html">ICCV19 Multi-Moments in Time (MIT) Challenge</a> (<a href="https://arxiv.org/abs/2003.05837">solutions</a>)</li>
</li>
<li><p>[2019] We obtain the <font color="#FF0000">Top-1</font> of <a href="https://www.kaggle.com/c/open-images-2019-instance-segmentation/leaderboard">ICCV19 OpenImage Instance Segmentation Challenge</a> (<a href="https://arxiv.org/abs/2003.07557">solutions</a>)</li>
</li>
<li><p>[2019] We obtain the <font color="#FF0000">Top-1</font> of <a href="https://www.kaggle.com/c/open-images-2019-object-detection/leaderboard">ICCV19 OpenImage Object Detection Challenge</a> (<a href="https://arxiv.org/abs/2003.07557">solutions</a>)[<a href="https://github.com/Sense-X/TSD">Code</a>]</li>
</li>
<li><p>[2019] We obtain the <font color="#FF0000">Top-1</font> of <a href="https://ibug.doc.ic.ac.uk/resources/lightweight-face-recognition-challenge-workshop/">ICCV19 Lightweight Face Recognition Challenge</a> (<a href="https://github.com/sciencefans/trojans-face-recognizer">model</a> and <a href="https://arxiv.org/pdf/1909.00632.pdf">report</a>)</li>


</ul>



<h2>About me</h2>
<p> With 8 years of experience in AI model research and development, I possess a keen technical insight into foundational large models and extensive hands-on experience in frontline R&D. <br />
</p>
<p>Director of the Base Model R&D Department and Director of the Base Model Services Department at SenseTime. <br />
A core member of the founding team for SenseTime's Visual Large Model. Part of the earliest team in China (2020) to engage with and take responsibility for large model training.<br />
 Led the frontline R&D and deployment of the Large Recognition Model (2020), Large Perception Model (2021), Large Multimodal Model (2021), and Large AIGC Model (2023-2024).</p>
<p>Managed a centralized platform team that supports over X production lines. This team has won the Group's highest research awards multiple times.</p>
<p>My research interests include: <b>large model design and optimization、 </b> <b>large AIGC model、</b> and <b>basic computer vision topics </b>(detection, classification, recognition, and video understanding).
<p>I also explore the design and optimization of the supervised learning in <a href="https://github.com/opendilab/DI-star" target=_blank>DI-star</a></p>

<h2>Working Experience</h2>
<ul>
    <li><p> Director of the Base Model R&D Department and the Base Model Services Department at <a href="https://www.sensetime.com" target=_blank>SenseTime BaseModel</a>. (2022 to Now) <br />
      Working on large AIGC model design and application.
  </p>
  </li>
  <li><p>Senior researcher at <a href="https://www.sensetime.com" target=_blank>SenseTime BaseModel</a>. (2021 to 2022) <br />
      Working on large model design and optimization.
  </p>
  </li>
<li><p>Researcher at <a href="https://www.sensetime.com" target=_blank>SenseTime X-Lab</a>. (2020 to 2021) <br />
    Working on large model design and optimization.
</p>
</li>
<li><p>Research intern at <a href="https://www.sensetime.com" target=_blank>SenseTime</a>. (2017 to 2020) <br />
Worked on object detection and recognition with <a href="https://liuyu.us/" target=_blank>Yu Liu</a>.</p>
</li>
</ul>




<h2>Publications </h2>
<br>*equal contribition, more publications please refer to <a href="https://scholar.google.com/citations?user=Bd3v08QAAAAJ&hl=zh-CN" target=_blank>Google Scholar</a>
<ul>

<li><p><a href="" target=_blank>ZoLA: Zero-Shot Creative Long Animation Generation with Short Video Model</a>  <br />
Fu Yun Wang, Zhaoyang Huang, Qiang Ma, <b>Guanglu Song</b>, Xudong LU, Weikang Bian, Yijin Li, Yu Liu, Hongsheng Li <br />
<i> ECCV2024</i></p>
</li>

<li><p><a href="" target=_blank>Three Things We Need to Know About Transferring Stable Diffusion to Visual Dense Prediction Tasks</a>  <br />
Manyuan Zhang, <b>Guanglu Song</b>, Xiaoyu Shi, Yu Liu, Hongsheng Li <br />
<i> ECCV2024</i></p>
</li>

<li><p><a href="" target=_blank>Deep reward supervisions for tuning text-to-image diffusion models</a>  <br />
Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, <b>Guanglu Song</b>, Yu Liu, Hongsheng Li <br />
<i> ECCV2024</i></p>
</li>

<li><p><a href="" target=_blank>Be-your-outpainter: Mastering video outpainting through input-specific adaptation</a>  <br />
Fu-Yun Wang, Xiaoshi Wu, Zhaoyang Huang, Xiaoyu Shi, Dazhong Shen, <b>Guanglu Song</b>, Yu Liu, Hongsheng Li <br />
<i> ECCV2024</i></p>
</li>

<li><p><a href="" target=_blank>AnimateLCM: Computation-Efficient Personalized Style Video Generation without Personalized Video Data</a>  <br />
Fu-Yun Wang, Zhaoyang Huang, Weikang Bian, Xiaoyu Shi, Keqiang Sun, <b>Guanglu Song</b>, Yu Liu, Hongsheng Li <br />
<i> SIGGRAPH Asia 2024 </i></p>
</li>

<li><p><a href="" target=_blank>Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models</a>  <br />
Bingqi Ma, Zhuofan Zong, <b>Guanglu Song</b>, Hongsheng Li, Yu Liu <br />
<i> NeurIPS2024 </i></p>
</li>

<li><p><a href="" target=_blank>Phased Consistency Model</a>  <br />
Fu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, <b>Guanglu Song</b>, Yu Liu, Hongsheng Li, Xiaogang Wang <br />
<i> NeurIPS2024 </i></p>
</li>

<li><p><a href="" target=_blank>Mova: Adapting mixture of vision experts to multimodal context</a>  <br />
Zhuofan Zong, Bingqi Ma, Dazhong Shen, <b>Guanglu Song</b>, Hao Shao, Dongzhi Jiang, Hongsheng Li, Yu Liu <br />
<i> NeurIPS2024 </i></p>
</li>

<li><p><a href="" target=_blank>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching</a>  <br />
Dongzhi Jiang, <b>Guanglu Song</b>, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, Hongsheng Li <br />
<i> NeurIPS2024 </i></p>
</li>

<li><p><a href="" target=_blank>Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning</a>  <br />
Hao Shao, Shengju Qian, Han Xiao, <b>Guanglu Song</b>, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li <br />
<i> NeurIPS2024 </i></p>
</li>

<li><p><a href="" target=_blank>FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis</a>  <br />
Linjiang Huang, Rongyao Fang, Aiping Zhang, <b>Guanglu Song</b>, Si Liu, Yu Liu, Hongsheng Li <br />
<i> ECCV2024 </i></p>
</li>

<li><p><a href="" target=_blank>Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance</a>  <br />
Dazhong Shen, <b>Guanglu Song*</b>, Zeyue Xue, Fu-Yun Wang, Yu Liu <br />
<i> CVPR2024 </i></p>
</li>

<li><p><a href="" target=_blank>RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths</a>  <br />
Zeyue Xue, <b>Guanglu Song</b>, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, Ping Luo <br />
<i> NeurIPS2023 </i></p>
</li>

<li><p><a href="" target=_blank>Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction</a>  <br />
Zhuofan Zong, Dongzhi Jiang, <b>Guanglu Song</b>, Zeyue Xue, Jingyong Su, Hongsheng Li, Yu Liu <br />
<i> ICCV2023 </i></p>
</li>

<li><p><a href="" target=_blank>UniFormer: Unifying Convolution and Self-Attention for Visual Recognition</a>  <br />
Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, <b>Guanglu Song</b>, Yu Liu, Hongsheng Li, Yu Qiao <br />
<i> TPAMI2023 </i></p>
</li>

<li><p><a href="" target=_blank>Rethinking Robust Representation Learning Under Fine-grained Noisy Faces</a>  <a href="">【Code (coming soon)】</a> <br />
Bingqi Ma, <b>Guanglu Song<sup>*</sup></b>, Boxiao Liu, Yu Liu <br />
<i> 2022 European Conference on Computer Vision (ECCV)</i></p>
</li>

<li><p><a href="" target=_blank>Unifying Visual Perception by Dispersible Points Learning</a>  <a href="https://github.com/Sense-X/UniHead">【Code】</a> <br />
Jianming Liang, <b>Guanglu Song</b>, Biao Leng, Yu Liu <br />
<i> 2022 European Conference on Computer Vision (ECCV)</i></p>
</li>

<li><p><a href="" target=_blank>Self-slimmed Vision Transformer</a>  <a href="https://github.com/Sense-X/SiT">【Code】</a> <br />
Zhuofan Zong, Kunchang Li, <b>Guanglu Song</b>, Yali Wang, Yu Qiao, Biao Leng, Yu Liu <br />
<i> 2022 European Conference on Computer Vision (ECCV)</i></p>
</li>

<li><p><a href="" target=_blank>UniNet: Unified Architecture Search with Convolution, Transformer, and MLP</a>  <a href="https://github.com/Sense-X/UniNet">【Code】</a> <br />
Jihao Liu, Xin Huang, <b>Guanglu Song</b>, Hongsheng Li, Yu Liu <br />
<i> 2022 European Conference on Computer Vision (ECCV)</i></p>
</li>

<li><p><a href="" target=_blank>Towards Robust Face Recognition with Comprehensive Search</a>  <a href="">【Code (coming soon)】</a> <br />
Manyuan Zhang, <b>Guanglu Song</b>, Yu Liu, Hongsheng Li <br />
<i> 2022 European Conference on Computer Vision (ECCV)</i></p>
</li>

<li><p><a href="https://arxiv.org/abs/2201.04676" target=_blank>UniFormer: Unified Transformer for Efficient Spatiotemporal Representation Learning</a> <a href="https://arxiv.org/pdf/2201.09450.pdf" target=_blank>【extended version】 </a> <a href="https://github.com/Sense-X/UniFormer">【Code】</a> <br />
Kunchang Li, Yali Wang, Peng Gao, <b>Guanglu Song</b>, Yu Liu, Hongsheng Li, Yu Qiao <br />
<i> 2022 ICLR</i></p>
</li>

<li><p><a href="https://arxiv.org/pdf/2111.08687.pdf" target=_blank>INTERN: A New Learning Paradigm Towards General Vision</a> <br />
Jing Shao, Siyu Chen, Yangguang Li, Kun Wang, Zhenfei Yin, Yinan He, Jianing Teng, Qinghong Sun, Mengya Gao, Jihao Liu, Gengshi Huang, <b>Guanglu Song</b>, Yichao Wu, Yuming Huang, Fenggang Liu, Huan Peng, Shuo Qin, Chengyu Wang, Yujie Wang, Conghui He, Ding Liang, Yu Liu, Fengwei Yu, Junjie Yan, Dahua Lin, Xiaogang Wang, Yu Qiao <br />
<i> Tech Report</i></p>
</li>

<li><p><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Switchable_K-Class_Hyperplanes_for_Noise-Robust_Representation_Learning_ICCV_2021_paper.pdf" target=_blank>Switchable K-class Hyperplanes for Noise-robust Representation Learning</a> <br />
Boxiao Liu<sup>*</sup>, <b>Guanglu Song<sup>*</sup></b>, Manyuan Zhang, Haihang You, Yu Liu <br />
<i> 2021 International Conference on Computer Vision (ICCV)</i></p>
</li>

<li><p><a href="https://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Liu_Rectifying_the_Data_Bias_in_Knowledge_Distillation_ICCVW_2021_paper.pdf" target=_blank>Rectifying the Data Bias in Knowledge Distillation</a> <br />
Boxiao Liu, Shenghan Zhang, <b>Guanglu Song</b>, Haihang You, Yu Liu <br />
<i><font color="#FF0000">(Best Workshop Paper)</font> 2021 International Conference on Computer Vision (ICCV) Masked Face Recognition Challenge & Workshop</i></p>
</li>


<li><p><a href="https://arxiv.org/abs/2008.10850" target=_blank>Discriminability Distillation in Group Representation Learning</a> <br />
Manyuan Zhang, <b>Guanglu Song</b>, Hang Zhou, Yu Liu<br />
<i> 2020 European Conference on Computer Vision (ECCV)</i></p>
</li>


<li><p><a href="https://arxiv.org/pdf/2003.07540.pdf" target=_blank>Revisiting the Sibling Head in Object Detector</a>, <a href="https://github.com/Sense-X/TSD">Code</a> <br />
<b>Guanglu Song</b>, Yu Liu, Xiaogang Wang <br />
<i><font color="#FF0000">(OpenImage 2019 Champion)</font> 2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i></p>
</li>


<li><p><a href="https://arxiv.org/abs/2003.07543" target=_blank>KPNet: Towards Minimal Face Detector</a> <br />
<b>Guanglu Song</b>, Yu Liu, Yuhang Zang, Xiaogang Wang, Biao Leng, Qingsheng Yuan <br />
<i><font color="#FF0000">(Oral)</font> 2020 AAAI Conference on Artificial Intelligence (AAAI)</i></p>
</li>


<li><p><a href="https://arxiv.org/pdf/1909.00632.pdf" target=_blank>Towards Flops-constrained Face Recognition</a>, <a href="https://github.com/sciencefans/trojans-face-recognizer">Code</a> <br />
Yu Liu*, <b>Guanglu Song*</b>, Manyuan Zhang*, Jihao Liu*, Yucong Zhou, Junjie Yan <br />
<i><font color="#FF0000">(Top-1 Solution)</font> 2019 ICCV Lightweight Face Recognition Challenge & Workshop</i></p>
</li>


<li><p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yu_Liu_Transductive_Centroid_Projection_ECCV_2018_paper.pdf" target=_blank>Transductive Centroid Projection for Semi-supervised Large-scale Recognition</a> <br />
Yu Liu, <b>Guanglu Song</b>, Jing Shao, Xiao Jin, Xiaogang Wang <br />
<i>2018 European Conference on Computer Vision (ECCV)</i></p>
</li>


<li><p><a href="https://arxiv.org/abs/1804.05197" target=_blank>Beyond Trade-off: Accelerate FCN-based Face Detector with Higher Accuracy</a> <br />
<b>Guanglu Song*</b>, Yu Liu*, Ming Jiang, Yujie Wang, Junjie Yan, Biao Leng <br>
<i>2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </i></p>
</li>


<li><p><a href="https://arxiv.org/abs/1711.08766" target=_blank>Region-based Quality Estimation Network for Large-Scale Person Re-identiﬁcation</a> <br />
<strong>Guanglu Song</strong>, Biao Leng, Yu Liu, Congrui Hetang, Shaofan Cai <br />
<i>2018 AAAI Conference on Artificial Intelligence (AAAI)</i></p>
</li>

<li><p><a href="https://arxiv.org/pdf/2003.05837" target=_blank>Top-1 Solution of Multi-Moments in Time Challenge 2019</a> <br />
Manyuan Zhang, Hao Shao, <b>Guanglu Song</b>, Yu Liu, Junjie Yan <br />
<i>Top-1 Solution of Multi-Moments in Time Challenge 2019</i></p>
</li>

<li><p><a href="https://arxiv.org/pdf/2003.07557" target=_blank>1st Place Solutions for OpenImage2019--Object Detection and Instance Segmentation</a> <br />
Yu Liu, <b>Guanglu Song</b>, Yuhang Zang, Yan Gao, Enze Xie, Junjie Yan, Chen Change Loy, Xiaogang Wang <br />
<i><font color="#FF0000">(Top-1 Solution)</font>1st Place Solutions for OpenImage2019--Object Detection and Instance Segmentation</i></p>
</li>

<li><p><a href="http://moments.csail.mit.edu/challenge2019/efficient_challenge_report.pdf" target=_blank>Team Efficient Multi-Moments in Time Challenge 2019 Technical Report</a> <br />
Manyuan Zhang, Hao Shao, <b>Guanglu Song</b>, Yu Liu, Junjie Yan <br />
<i><font color="#FF0000">(Top-1 Solution)</font>Team Efficient Multi-Moments in Time Challenge 2019 Technical Report</i></p>
</li>

</ul>


<h2>Projects & Datasets</h2>
<ul>
<li><p><a href="https://github.com/Sense-X/TSD">TSD</a>, OpenImage Top-1 solutions.</p>
</li>
<li><p><a href="dataset/lpw/index.html">Labeled Pedestrains in the Wild </a>，a large scale pedestrain re-identification benchmark</p>
</li>
</ul>


<!-- <h2>Academic Services</h2>
<p>Reviewer of the following conferences and journals:</p>
<ul>
<li><p>CVPR 2019 <b>(Outstanding Reviewer)</b>, ICCV 2019, AAAI 2019.</p>
</li>
<li><p>IJCV, TCSVT</p>
</li>
</ul> -->

<!--
<h2>Teaching Experience</h2>
<p>Teaching Assistant of the following courses in The Chinese University of Hong Kong:</p>
<ul>
<li><p>ENGG 5202, <b>Pattern Recognition</b>, Fall 2017.</p>
</li>
<li><p>ENGG 2450A, <b>Probability and Statistics for Engineers</b>, Spring 2018.</p>
</li>
<li><p>Summer Tutorial, <b>Potential Inspiration in Electronic Engineering</b>, Summer 2018.</p>
</li>
<li><p>ELEG 5760, <b>Machine Learning for Signal Processing Applications</b>, Fall 2018.</p>
</li>
<li><p>ELEG 5491, <b>Introduction to Deep Learning</b> <a href="http://dl.ee.cuhk.edu.hk" target=_blank>[Course website]</a>, Spring 2019. </p>
</li>
<li><p>ENGG 2420B, <b>Complex Analyis and Differential Equations for Engineers</b>, Fall 2019. </p>
</li>
</ul>
 -->

</td>
</tr>
</table>
</body>
</html>
